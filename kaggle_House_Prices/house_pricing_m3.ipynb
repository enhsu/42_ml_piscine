{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import things\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_data:\n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.missing_data = self.get_missing()\n",
    "        \n",
    "    def get_missing(self):\n",
    "        total = self.data.isnull().sum().sort_values(ascending=False)\n",
    "        percent = (self.data.isnull().sum() / self.data.isnull().count()).sort_values(ascending=False)\n",
    "        missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "        return missing_data\n",
    "        \n",
    "    def chose_model(self, model, using_col, cat_col, is_test=False):\n",
    "        if model == 1:\n",
    "            self.model_df = self.drop_nan_data(self.missing_data)\n",
    "        elif model == 2:\n",
    "            self.model_df = self.deal_nan_data()\n",
    "        elif model == 3:\n",
    "            self.model_df = self.deal_ol_data()\n",
    "        else:\n",
    "            self.model_df = self.handle()\n",
    "            print('not build yet.')\n",
    "        \n",
    "        # Id for combine two dataframe  \n",
    "        id_lst = self.model_df['Id']\n",
    "        # deal with SalePrice\n",
    "        if not is_test:\n",
    "            house_price = self.model_df['SalePrice']\n",
    "    #       house_price = np.log(house_price)\n",
    "            house_price /= 1000\n",
    "        # scaler value\n",
    "        tmp1_df = self.model_df[using_col]\n",
    "        tmp1_df = (tmp1_df - tmp1_df.mean()) / tmp1_df.std()\n",
    "        if not is_test:\n",
    "            tmp1_df['SalePrice'] = house_price\n",
    "        tmp1_df['Id'] = id_lst\n",
    "        # category value\n",
    "        tmp2_df = self.model_df[cat_col]\n",
    "        # merge scaler value df & category value df\n",
    "        self.model_df = pd.merge(tmp1_df, tmp2_df, on='Id')\n",
    "        self.model_df = self.model_df.drop(['Id'], axis=1)\n",
    "        # one-hot encode\n",
    "        self.model_df = self.dum_data(self.model_df)\n",
    "        \n",
    "    def drop_nan_data(self, missing_df):\n",
    "        drop_data = self.data.copy()\n",
    "        drop_data = drop_data.drop((missing_df[missing_df['Total'] > 1]).index, 1)\n",
    "        drop_data = drop_data.drop(drop_data.loc[drop_data['Electrical'].isnull()].index)\n",
    "        self.drop_data = drop_data\n",
    "        return drop_data\n",
    "    \n",
    "    def deal_nan_data(self):\n",
    "        fill_data = self.data.copy()\n",
    "        fill_data['BsmtQual'] = fill_data['BsmtQual'].apply(lambda x: 'NOVAL' if x != x else x)\n",
    "        self.fill_data = fill_data\n",
    "        return fill_data\n",
    "        \n",
    "    def deal_ol_data(self):\n",
    "        ol_data = self.data.copy()\n",
    "        ol_data['1stFlrSF'] = ol_data['1stFlrSF'].apply(lambda x: min(x, 2500))\n",
    "        ol_data['GrLivArea'] = ol_data['GrLivArea'].apply(lambda x: min(x, 3000))\n",
    "        ol_data['TotalBsmtSF'] = ol_data['TotalBsmtSF'].apply(lambda x: min(x, 3000))\n",
    "        self.ol_data = ol_data\n",
    "        return ol_data\n",
    "    \n",
    "    def dum_data(self, data_df):\n",
    "        return pd.get_dummies(data_df)\n",
    "#     def out_liars(self):\n",
    "    \n",
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df,\n",
    "        label_col='SalePrice',\n",
    "        learning_rate=0.02,\n",
    "        steps=100,\n",
    "        batch_size=10,\n",
    "        periods=10,\n",
    "        hidden_units=[8, 4],\n",
    "        load_model=False,\n",
    "        load_model_name='default',\n",
    "        save_model=False,\n",
    "        save_model_name='default',\n",
    "    ):\n",
    "        self.label = label_col\n",
    "        # learning rate: optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        # steps, batch_size, periods: train\n",
    "        self.steps = steps\n",
    "        self.batch_size = batch_size\n",
    "        self.periods = periods\n",
    "        self.h_units = hidden_units\n",
    "        # save model\n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_name = save_model_name\n",
    "        self.load_model_name = load_model_name\n",
    "        self.save_model_path = './model/' + self.save_model_name\n",
    "        self.load_model_path = './model/' + self.load_model_name\n",
    "        # split data\n",
    "        self.te, self.tt, self.ve, self.vt = self.get_split_data(train_df)\n",
    "        \n",
    "        # create lenearRegressor\n",
    "        self.feature_cols = self.get_feature_cols(train_df)\n",
    "        self.optimizer = self.get_optimizer(learning_rate)\n",
    "        \n",
    "        if self.load_model:\n",
    "            if self.save_model:\n",
    "                self.lr = tf.estimator.DNNRegressor(\n",
    "                    hidden_units=self.h_units,\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    model_dir=self.save_model_path,\n",
    "                    warm_start_from=self.load_model_path\n",
    "                )\n",
    "            else:\n",
    "                self.lr = tf.estimator.DNNRegressor(\n",
    "                    hidden_units=self.h_units,\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    warm_start_from=self.load_model_path\n",
    "                )\n",
    "        else:\n",
    "            if self.save_model:\n",
    "                self.lr = tf.estimator.DNNRegressor(\n",
    "                    hidden_units=self.h_units,\n",
    "                #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    model_dir=self.save_model_path\n",
    "                ) \n",
    "            else:\n",
    "                self.lr = tf.estimator.DNNRegressor(\n",
    "                    hidden_units=[20, 10, 5],\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer\n",
    "                )\n",
    "        print('build the model')\n",
    "    \n",
    "    def get_split_data(self, train_df):\n",
    "        # sample 80% for train data, 20% for vali data\n",
    "        train_set, vali_set = self.split_train(train_df, 0.8)\n",
    "        te, tt = self.get_e_t(train_set)\n",
    "        ve, vt = self.get_e_t(vali_set)\n",
    "        return te, tt, ve, vt\n",
    "        \n",
    "    def split_train(self, data_df, per):\n",
    "        t_s = data_df.sample(frac=per, replace=False, random_state=42)\n",
    "        v_s = data_df.loc[ set(data_df.index) - set(t_s.index)]\n",
    "        return t_s, v_s\n",
    "    \n",
    "    def get_e_t(self, data_df):\n",
    "        # data examples\n",
    "        d_e = data_df.copy().drop(self.label, axis=1)\n",
    "        # data targets\n",
    "        d_t = pd.DataFrame()\n",
    "        d_t[self.label] = data_df[self.label]\n",
    "        return d_e, d_t\n",
    "    \n",
    "    def get_feature_cols(self, train_df):\n",
    "        feature_df = train_df.copy().drop(self.label, axis=1)\n",
    "        tmp_feature = [tf.feature_column.numeric_column(my_feature) for my_feature in feature_df]\n",
    "#         if bin_age: # boolean:\n",
    "#             tmp_age = tf.feature_column.numeric_column(\"Age\")\n",
    "#             bucketized_age = tf.feature_column.bucketized_column(\n",
    "#               tmp_age, boundaries=get_quantile_based_boundaries(\n",
    "#                 input_features[\"Age\"], 4))\n",
    "#             tmp_feature += [bucketized_age]\n",
    "        return set(tmp_feature)\n",
    "    \n",
    "    def get_optimizer(self, learning_rate):\n",
    "#         change the optimizer!!!!!!\n",
    "        my_opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#         my_opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        my_opt = tf.contrib.estimator.clip_gradients_by_norm(my_opt, 2.0)\n",
    "        return my_opt\n",
    "    \n",
    "    def my_input_fn(\n",
    "        self,\n",
    "        features,\n",
    "        targets,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_epochs=None\n",
    "    ):\n",
    "        # Convert pandas data into a dict of np arrays.\n",
    "        features = {key:np.array(value) for key,value in dict(features).items()}\n",
    "        # Construct a dataset, and configure batching/repeating.\n",
    "        ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        # Shuffle the data, if specified.\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "        # Return the next batch of data.\n",
    "        features, labels = ds.make_one_shot_iterator().get_next()\n",
    "        return features, labels\n",
    "    \n",
    "    def train(self):\n",
    "        steps_per_period = self.steps / self.periods\n",
    "        # create input function\n",
    "        training_input_fn = lambda: self.my_input_fn(self.te, self.tt[self.label], batch_size=self.batch_size)\n",
    "        predict_training_input_fn = lambda: self.my_input_fn(self.te, self.tt[self.label], num_epochs=1, shuffle=False)\n",
    "        predict_vali_input_fn = lambda: self.my_input_fn(self.ve, self.vt[self.label], num_epochs=1, shuffle=False)\n",
    "        \n",
    "        print('Training model...')\n",
    "        # recording RMSE\n",
    "        training_rmse = []\n",
    "        validation_rmse = []\n",
    "        for period in range(self.periods):\n",
    "            self.lr.train(\n",
    "                input_fn=training_input_fn,\n",
    "                steps=steps_per_period\n",
    "            )\n",
    "            \n",
    "            # compute training predictions\n",
    "            training_predictions = self.lr.predict(input_fn=predict_training_input_fn)\n",
    "            training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "            # compute validation predictions\n",
    "            validation_predictions = self.lr.predict(input_fn=predict_vali_input_fn)\n",
    "            validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "            # get validation eval\n",
    "            training_eval = self.lr.evaluate(input_fn=predict_training_input_fn)\n",
    "            validation_eval = self.lr.evaluate(input_fn=predict_vali_input_fn)\n",
    "            print(\"training_eval: {}\".format(training_eval['average_loss']))\n",
    "            print(\"validation_eval: {}\".format(validation_eval['average_loss']))\n",
    "\n",
    "            # compute training loss\n",
    "            training_root_mean_squared_error = math.sqrt(\n",
    "                metrics.mean_squared_error(training_predictions, self.tt))\n",
    "            # compute validation loss\n",
    "            validation_root_mean_squared_error = math.sqrt(\n",
    "                metrics.mean_squared_error(validation_predictions, self.vt))\n",
    "            # Occasionally print the current loss.\n",
    "            print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
    "            # Add the loss metrics from this period to our list.\n",
    "            training_rmse.append(training_root_mean_squared_error)\n",
    "            validation_rmse.append(validation_root_mean_squared_error)\n",
    "        \n",
    "        print(\"Model training finished.\")\n",
    "        # saving model\n",
    "        if self.save_model:\n",
    "            print(\"Saving model...\")\n",
    "            \n",
    "        # output a graph of loss metrics over periods.\n",
    "        self.result_plot(training_rmse, validation_rmse)\n",
    "        \n",
    "    def result_plot(self, t_rmse, v_rmse):\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.xlabel(\"Periods\")\n",
    "        plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "        plt.tight_layout()\n",
    "        plt.plot(t_rmse, label=\"training\")\n",
    "        plt.plot(v_rmse, label=\"validation\")\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "class predict_file:\n",
    "    def __init__(self, model, test_df, id_frame):\n",
    "        self.test_df = test_df\n",
    "        self.id_frame = id_frame\n",
    "#         self.predict_col = predict_col\n",
    "        self.predict_input_fn = self.create_predict_fn()\n",
    "        \n",
    "        self.predictions = list(model.lr.predict(input_fn=self.predict_input_fn))\n",
    "        self.predictions = np.array([item['predictions'][0] for item in self.predictions])\n",
    "        self.predictions *= 1000\n",
    "        \n",
    "        self.evaluation = self.id_frame#test_df['Id'].copy().to_frame()\n",
    "        self.evaluation['SalePrice'] = self.predictions\n",
    "    \n",
    "    def create_predict_fn(self):\n",
    "#         predict_df = self.test_df\n",
    "        predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "            x=self.test_df,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return predict_input_fn\n",
    "    \n",
    "    def save_predict(self, path):\n",
    "        self.evaluation.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "build the model\n"
     ]
    }
   ],
   "source": [
    "# model 3: deal with NaN data & out lier data\n",
    "# using_col = ['SalePrice', 'OverallQual', 'TotalBsmtSF', YearBuilt']\n",
    "m3_using_col = ['Id', 'SalePrice', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath', '1stFlrSF']\n",
    "m3_category_col = ['Id', 'SaleType', 'SaleCondition']\n",
    "# predict_col = using_col[1:]\n",
    "\n",
    "# loading csv file\n",
    "train_df = create_data(train_path)\n",
    "\n",
    "# deal out lier data\n",
    "train_df.chose_model(3, m3_using_col, m3_category_col)\n",
    "train_df.model_df.head()\n",
    "\n",
    "# create model\n",
    "model3 = create_model(\n",
    "    train_df=train_df.model_df, \n",
    "#     save_model=True, \n",
    "    save_model_name=\"model3_test_v1\",\n",
    "    load_model=True, \n",
    "    load_model_name=\"model3_test_v1\",\n",
    "    learning_rate=0.08,\n",
    "    steps=200,\n",
    "    batch_size=15,\n",
    "#     hidden_units=[3]\n",
    ")\n",
    "\n",
    "# training model\n",
    "# model3.train()\n",
    "\n",
    "\n",
    "# data = pd.concat([t_df.data['SalePrice'], t_df.data['GrLivArea']], axis=1)\n",
    "# data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))\n",
    "\n",
    "# data2 = pd.concat([t_df.ol_data['SalePrice'], t_df.ol_data['GrLivArea']], axis=1)\n",
    "# data2.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = create_data(test_path)\n",
    "m1_using_col = ['Id', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath', '1stFlrSF']\n",
    "m1_category_col = ['Id', 'SaleType', 'SaleCondition']\n",
    "test_df.chose_model(1, m1_using_col, m1_category_col, True)\n",
    "id_frame = test_df.data['Id'].copy().to_frame()\n",
    "\n",
    "predict = predict_file(model3, test_df.model_df, id_frame)\n",
    "# predict.evaluation.head()\n",
    "predict.save_predict('m3_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
