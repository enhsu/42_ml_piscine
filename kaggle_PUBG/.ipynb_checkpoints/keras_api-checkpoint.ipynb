{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# for tensorboard\n",
    "import os\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, PReLU, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "train_path = './data/train_V2.csv'\n",
    "test_path = './data/test_V2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, is_train=True):\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        if is_train:\n",
    "            self.feature = data.drop('winPlacePerc', axis=1)\n",
    "            self.label = data['winPlacePerc']\n",
    "        else:\n",
    "            self.feature = data\n",
    "        del data\n",
    "        \n",
    "        self.deal_feature()\n",
    "        if is_train:\n",
    "            self.ts_f, self.ts_l, self.vs_f, self.vs_l = self.split_t_v()\n",
    "        \n",
    "    def deal_feature(self):\n",
    "        self.fillInf(0)\n",
    "        self.ohencode()\n",
    "        self.drop_id()\n",
    "            \n",
    "    def fillInf(self, val):\n",
    "        numcols = self.feature.select_dtypes(include='number').columns\n",
    "        cols = numcols[numcols != 'winPlacePerc']\n",
    "        self.feature[self.feature == np.Inf] = np.NaN\n",
    "        self.feature[self.feature == np.NINF] = np.NaN\n",
    "        for c in cols:\n",
    "            self.feature[c].fillna(val, inplace=True)\n",
    "        \n",
    "    def ohencode(self):\n",
    "        '''\n",
    "        solo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\n",
    "        duo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\n",
    "        squad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n",
    "        '''\n",
    "        mapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n",
    "        self.feature['matchType'] = self.feature['matchType'].apply(mapper)\n",
    "\n",
    "        self.feature = pd.concat([self.feature, pd.get_dummies(self.feature['matchType'], prefix='matchType')], axis=1)\n",
    "    \n",
    "    def drop_id(self):\n",
    "        self.feature = self.feature.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1)\n",
    "        \n",
    "    def split_t_v(self):\n",
    "        ts_f, vs_f, ts_l, vs_l = \\\n",
    "        train_test_split(\n",
    "            self.feature,\n",
    "            self.label,\n",
    "            test_size=0.1,\n",
    "            random_state=2\n",
    "        )\n",
    "        return ts_f, ts_l, vs_f, vs_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        print(logs)\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "#         learning_rate=0.001,\n",
    "        batch_size=42,\n",
    "        epochs=10,\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        save_model_name='default',\n",
    "        load_model_name='default',\n",
    "        tensorboard=True,\n",
    "    ):\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "#         self.lr = learning_rate\n",
    "        \n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_json_path = './model/' + save_model_name + '.json'\n",
    "        self.save_model_HDF5_path = './model/' + save_model_name + '.h5'\n",
    "        self.load_model_json_path = './model/' + load_model_name + '.json'\n",
    "        self.load_model_HDF5_path = './model/' + load_model_name + '.h5'\n",
    "        self.has_tb = tensorboard\n",
    "        # tensorboard\n",
    "        if self.has_tb:\n",
    "            log_dir = './tensorboard/{}'.format(time())\n",
    "            self.tensorboard = TrainValTensorBoard(log_dir=log_dir, write_graph=False)\n",
    "        # loading model | create model\n",
    "        if load_model:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.buildDNN()\n",
    "        # optimizer & model compile\n",
    "        self.create_optimizer()\n",
    "        self.model_compile()\n",
    "        print('create model.')\n",
    "        \n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.save_model_json_path, 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(self.save_model_HDF5_path)\n",
    "        print(\"Saving the model...\")\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            json_file = open(self.load_model_json_path, 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(self.load_model_HDF5_path)\n",
    "            print(\"Loaded model...\")\n",
    "        except:\n",
    "            print(\"Loading error!\")\n",
    "            \n",
    "        \n",
    "    def buildDNN(self):            \n",
    "#         self.model.add(Flatten())\n",
    "        self.model.add(Dense(512, kernel_initializer='he_normal', input_dim=self.input_shape, activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "        self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "        self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.1))\n",
    "\n",
    "        self.model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "        \n",
    "\n",
    "    def model_compile(self):\n",
    "        self.model.compile(\n",
    "            optimizer=self.adam,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        self.adam = keras.optimizers.Adam(lr=0.005)\n",
    "    \n",
    "    def train(self, ts_f, ts_l, vs_f, vs_l):\n",
    "        self.create_callbacks()\n",
    "        \n",
    "        cbs = [self.lr_sched, self.early_stopping]\n",
    "        if self.has_tb:\n",
    "            cbs.append(self.tensorboard)\n",
    "        \n",
    "        print('start training...')\n",
    "        self.history = self.model.fit(\n",
    "            ts_f,\n",
    "            ts_l,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=cbs,\n",
    "#             verbose=2\n",
    "        )\n",
    "    \n",
    "        self.result = self.model.evaluate(x=vs_f, y=vs_l)\n",
    "        print('Accuracy:', self.result[1])\n",
    "        if self.save_model:\n",
    "            self.save()\n",
    "        self.print_plot()\n",
    "    \n",
    "    def create_callbacks(self):\n",
    "        self.lr_sched = self.step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=1)\n",
    "        self.early_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode='min', patience=10, verbose=1)\n",
    "        \n",
    "    def step_decay_schedule(self, initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
    "        ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n",
    "        def schedule(epoch):\n",
    "            return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "\n",
    "        return LearningRateScheduler(schedule, verbose)\n",
    "    \n",
    "    def print_plot(self):\n",
    "        self.print_loss()\n",
    "        self.print_acc()\n",
    "    \n",
    "    def print_loss(self):\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.title(\"Model Loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Test'])\n",
    "        plt.show()\n",
    "        \n",
    "    def print_acc(self):\n",
    "        plt.plot(self.history.history['mean_absolute_error'])\n",
    "        plt.plot(self.history.history['val_mean_absolute_error'])\n",
    "        plt.title('Mean Abosulte Error')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Mean absolute error')\n",
    "        plt.legend(['Train','Test'])\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, test):\n",
    "        pred_digits_test = np.argmax(self.model.predict(test), axis=1)\n",
    "        image_id_test=[]\n",
    "        for i in range(len(pred_digits_test)):\n",
    "            image_id_test.append(i+1)\n",
    "        d = {\n",
    "            'ImageId': image_id_test,\n",
    "            'Label': pred_digits_test\n",
    "        }\n",
    "        this.answer = pd.DataFrame(d)\n",
    "        \n",
    "    def save_pred(self, path):\n",
    "        this.answer.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "(4002269, 27)\n",
      "(4002269,)\n",
      "(444697, 27)\n",
      "(444697,)\n",
      "create model.\n"
     ]
    }
   ],
   "source": [
    "pubg = get_data(train_path)\n",
    "print(pubg.ts_f.shape[1])\n",
    "print(pubg.ts_f.shape)\n",
    "print(pubg.ts_l.shape)\n",
    "print(pubg.vs_f.shape)\n",
    "print(pubg.vs_l.shape)\n",
    "model = create_model(\n",
    "    input_shape=pubg.ts_f.shape[1]\n",
    "#     batch_size=10,\n",
    "#     num_classes=10,\n",
    "#     epochs=2,\n",
    "#     save_model=True,\n",
    "#     load_model=True,\n",
    "#     save_model_name='default',\n",
    "#     load_model_name='default',\n",
    "#     tensorboard=True\n",
    ")\n",
    "\n",
    "# Error when checking target: expected dense_12 to have shape (1,) but got array with shape (28,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Train on 3201815 samples, validate on 800454 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      " 245280/3201815 [=>............................] - ETA: 7:50 - loss: 0.0176 - mean_absolute_error: 0.0986"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f564199fd4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mts_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpubg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvs_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpubg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mvs_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpubg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-44-30727cbb92d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ts_f, ts_l, vs_f, vs_l)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;31m#             verbose=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    ts_f=pubg.ts_f,\n",
    "    ts_l=pubg.ts_l,\n",
    "    vs_f=pubg.vs_f,\n",
    "    vs_l=pubg.vs_l\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
