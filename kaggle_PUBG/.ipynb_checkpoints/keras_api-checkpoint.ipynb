{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d0e4a4a6a0f08ac241b4707efbd7ba66919edff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "train_path = './data/train_V2.csv'\n",
    "test_path = './data/test_V2.csv'\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "564ed6f6351cea755f1854b5320b83cbd05ded97"
   },
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        if is_train:\n",
    "            self.feature = data.drop('winPlacePerc', axis=1)\n",
    "            self.label = data['winPlacePerc']\n",
    "        else:\n",
    "            self.feature = data\n",
    "        del data\n",
    "        \n",
    "        self.deal_feature()\n",
    "        self.x_train = self.feature\n",
    "        if is_train:\n",
    "            self.y_train = self.label\n",
    "#         if is_train:\n",
    "#             self.ts_f, self.ts_l, self.vs_f, self.vs_l = self.split_t_v()\n",
    "        \n",
    "    def deal_feature(self):\n",
    "        self.add_cols()\n",
    "        self.fillna()\n",
    "        self.drop_cols()\n",
    "#         self.featuring()\n",
    "#         self.ohencode()\n",
    "        self.drop_ids()\n",
    "            \n",
    "    def add_cols(self):\n",
    "        self.feature['teamPlayers'] = self.feature['groupId'].map(self.feature['groupId'].value_counts())\n",
    "        self.feature['gamePlayers'] = self.feature['matchId'].map(self.feature['matchId'].value_counts())\n",
    "        self.feature['enemyPlayers'] = self.feature['gamePlayers'] - self.feature['teamPlayers']\n",
    "        self.feature['totalDistance'] = self.feature['rideDistance'] + self.feature['swimDistance'] + self.feature['walkDistance']\n",
    "        self.feature['enemyDamage'] = self.feature['assists'] + self.feature['kills']\n",
    "        \n",
    "        totalKills = self.feature.groupby(['matchId', 'groupId']).agg({'kills': lambda x: x.sum()})\n",
    "        totalKills.rename(columns={'kills': 'squadKills'}, inplace=True)\n",
    "        self.feature = self.feature.join(other=totalKills, on=['matchId', 'groupId'])\n",
    "        \n",
    "        self.feature['medicKits'] = self.feature['heals'] + self.feature['boosts']\n",
    "        self.feature['medicPerKill'] = self.feature['medicKits'] / self.feature['enemyDamage']\n",
    "        self.feature['distancePerHeals'] = self.feature['totalDistance'] / self.feature['heals']\n",
    "        self.feature['headShotKillRatio'] = self.feature['headshotKills'] / self.feature['kills']\n",
    "        self.feature['headshotKillRate'] = self.feature['headshotKills'] / self.feature['kills']\n",
    "        self.feature['killPlaceOverMaxPlace'] = self.feature['killPlace'] / self.feature['maxPlace']\n",
    "        self.feature['kills/walkDistance'] = self.feature['kills'] / self.feature['walkDistance']\n",
    "        self.feature['avgKills'] = self.feature['squadKills'] / self.feature['teamPlayers']\n",
    "        self.feature['damageRatio'] = self.feature['damageDealt'] / self.feature['enemyDamage']\n",
    "        self.feature['distTravelledPerGame'] = self.feature['totalDistance'] / self.feature['matchDuration']\n",
    "        self.feature['killPlacePerc'] = self.feature['killPlace'] / self.feature['gamePlayers']\n",
    "        self.feature['playerSkill'] = self.feature['headshotKills'] + self.feature['roadKills'] + self.feature['assists'] - (5 * self.feature['teamKills'])\n",
    "        self.feature['gamePlacePerc'] = self.feature['killPlace'] / self.feature['maxPlace']\n",
    "    \n",
    "    def fillna(self):\n",
    "        self.feature.fillna(0, inplace=True)\n",
    "        self.feature.replace(np.inf, 0, inplace=True)\n",
    "        if self.is_train:\n",
    "            self.label.fillna(0, inplace=True)\n",
    "            self.label.replace(np.inf, 0, inplace=True)\n",
    "            \n",
    "#     def fillInf(self, val):\n",
    "#         numcols = self.feature.select_dtypes(include='number').columns\n",
    "#         cols = numcols[numcols != 'winPlacePerc']\n",
    "#         self.feature[self.feature == np.Inf] = np.NaN\n",
    "#         self.feature[self.feature == np.NINF] = np.NaN\n",
    "#         for c in cols:\n",
    "#             self.feature[c].fillna(val, inplace=True)\n",
    "        \n",
    "    def ohencode(self):\n",
    "        '''\n",
    "        solo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\n",
    "        duo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\n",
    "        squad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n",
    "        '''\n",
    "        mapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n",
    "        self.feature['matchType'] = self.feature['matchType'].apply(mapper)\n",
    "\n",
    "        self.feature = pd.concat([self.feature, pd.get_dummies(self.feature['matchType'], prefix='matchType')], axis=1)\n",
    "    \n",
    "    def drop_cols(self):\n",
    "        drop_cols = ['killPoints', 'rankPoints', 'winPoints', 'maxPlace']\n",
    "        self.feature.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    def drop_ids(self):\n",
    "        self.feature = self.feature.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1)\n",
    "    \n",
    "    def featuring(self):\n",
    "        features = list(self.feature.columns)\n",
    "        features.remove(\"Id\")\n",
    "        features.remove(\"matchId\")\n",
    "        features.remove(\"groupId\")\n",
    "        features.remove(\"matchType\")\n",
    "        condition='False'\n",
    "        \n",
    "        if 'winPlacePerc' in self.feature.columns:\n",
    "            y = np.array(self.feature.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean'), dtype=np.float64)\n",
    "            features.remove(\"winPlacePerc\")\n",
    "            condition='True'\n",
    "        \n",
    "        # get group mean feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = agg.reset_index()[['matchId','groupId']]\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "        # get group max feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('max')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "        \n",
    "        # get group min feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('min')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "        \n",
    "        # get match mean feature\n",
    "        agg = self.feature.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "        df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "        df_id=df_out[[\"matchId\", \"groupId\"]].copy()\n",
    "        df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "        del agg, agg_rank\n",
    "        gc.collect()\n",
    "        if condition == 'True':\n",
    "            return df_out,pd.DataFrame(y),df_id\n",
    "        else:\n",
    "            return df_out,df_id\n",
    "        \n",
    "    def split_t_v(self):\n",
    "        ts_f, vs_f, ts_l, vs_l = \\\n",
    "        train_test_split(\n",
    "            self.feature,\n",
    "            self.label,\n",
    "            test_size=0.1,\n",
    "            random_state=2\n",
    "        )\n",
    "        return ts_f, ts_l, vs_f, vs_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05e833466e673124bea8622cde2356fa478bfcd4"
   },
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        print(logs)\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8e1730b1c8ed553148bf3da226660a97619249d"
   },
   "outputs": [],
   "source": [
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape,\n",
    "        epochs=100,\n",
    "        batch_size=100000,\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        save_model_name='test',\n",
    "        load_model_name='test',\n",
    "        tensorboard=False,\n",
    "    ):\n",
    "        self.shape=shape\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_json_path = './model/' + save_model_name + '.json'\n",
    "        self.save_model_HDF5_path = './model/' + save_model_name + '.h5'\n",
    "        self.load_model_json_path = './model/' + load_model_name + '.json'\n",
    "        self.load_model_HDF5_path = './model/' + load_model_name + '.h5'\n",
    "        self.has_tb = tensorboard\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.build_NN()\n",
    "        self.compile_model()\n",
    "        \n",
    "        # tensorboard\n",
    "        if self.has_tb:\n",
    "            log_dir = './tensorboard/{}'.format(time())\n",
    "            self.tensorboard = TrainValTensorBoard(log_dir=log_dir, write_graph=False)\n",
    "        \n",
    "    def build_NN(self):\n",
    "        self.model.add(Dense(80,input_dim=self.shape,activation='selu'))\n",
    "        self.model.add(Dense(160,activation='selu'))\n",
    "        self.model.add(Dense(320,activation='selu'))\n",
    "        self.model.add(Dropout(0.1))\n",
    "        self.model.add(Dense(160,activation='selu'))\n",
    "        self.model.add(Dense(80,activation='selu'))\n",
    "        self.model.add(Dense(40,activation='selu'))\n",
    "        self.model.add(Dense(20,activation='selu'))\n",
    "        self.model.add(Dense(1,activation='sigmoid'))\n",
    "        \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.history = self.model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "#             callbacks=[self.tensorboard]\n",
    "        )\n",
    "        if self.save_model:\n",
    "            self.save()\n",
    "        \n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.save_model_json_path, 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(self.save_model_HDF5_path)\n",
    "        print(\"Saving the model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71b0c052d732431e4298e309d684ea9d51e0cf01"
   },
   "outputs": [],
   "source": [
    "pg_data = get_data(train_path)\n",
    "pg_model = create_model(shape=pg_data.x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07d485d53e5e7fca1a7f26de5f89dfe0cde073d5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pg_model.train(pg_data.x_train, pg_data.y_train)\n",
    "# pg_data.x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "667bfa49272fee7c16eb6e959dd4b81ce9677cc1"
   },
   "outputs": [],
   "source": [
    "pg_test = get_data(test_path, is_train=False)\n",
    "prediction = pg_model.model.predict(pg_test.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e55a815c3305347530458aa1353c516b6d77b3ea"
   },
   "outputs": [],
   "source": [
    "prediction_ravel = prediction.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48ee7e569c3a8a3b414a3ca2ec7f77ed5a2b9eb6"
   },
   "outputs": [],
   "source": [
    "prediction_ser = pd.Series(prediction_ravel, name='winPlacePerc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d07830efbf57236b204acda21ae2d0741737b1b"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "deadd3fa46eec982f7b8eec1be620dc7591beeb8"
   },
   "outputs": [],
   "source": [
    "submit['winPlacePerc'] = prediction_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78eb9070bd2252700f2606a5ede2baa328fcd922"
   },
   "outputs": [],
   "source": [
    "act_sub = submit[['Id', 'winPlacePerc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80f03c9e316fc0508ae0fc114a70bacfe20b7459"
   },
   "outputs": [],
   "source": [
    "act_sub.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4019f40a22a319e423f4c0beb4c90ad83145e3e8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
