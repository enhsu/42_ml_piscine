{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_path = './data/train_V2.csv'\n",
    "test_path = './data/test_V2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        if is_train:\n",
    "            self.feature = data.drop('winPlacePerc', axis=1)\n",
    "            self.label = data['winPlacePerc']\n",
    "        else:\n",
    "            self.feature = data\n",
    "        del data\n",
    "        \n",
    "        self.deal_feature()\n",
    "        self.x_train = self.feature\n",
    "        if is_train:\n",
    "            self.y_train = self.label\n",
    "        if is_train:\n",
    "            self.ts_f, self.ts_l, self.vs_f, self.vs_l = self.split_t_v()\n",
    "        \n",
    "    def deal_feature(self):\n",
    "        self.add_cols()\n",
    "        self.fillna()\n",
    "        self.drop_cols()\n",
    "#         self.featuring()\n",
    "#         self.ohencode()\n",
    "        self.drop_ids()\n",
    "            \n",
    "    def add_cols(self):\n",
    "        self.feature['teamPlayers'] = self.feature['groupId'].map(self.feature['groupId'].value_counts())\n",
    "        self.feature['gamePlayers'] = self.feature['matchId'].map(self.feature['matchId'].value_counts())\n",
    "        self.feature['enemyPlayers'] = self.feature['gamePlayers'] - self.feature['teamPlayers']\n",
    "        self.feature['totalDistance'] = self.feature['rideDistance'] + self.feature['swimDistance'] + self.feature['walkDistance']\n",
    "        self.feature['enemyDamage'] = self.feature['assists'] + self.feature['kills']\n",
    "        \n",
    "        totalKills = self.feature.groupby(['matchId', 'groupId']).agg({'kills': lambda x: x.sum()})\n",
    "        totalKills.rename(columns={'kills': 'squadKills'}, inplace=True)\n",
    "        self.feature = self.feature.join(other=totalKills, on=['matchId', 'groupId'])\n",
    "        \n",
    "        self.feature['medicKits'] = self.feature['heals'] + self.feature['boosts']\n",
    "        self.feature['medicPerKill'] = self.feature['medicKits'] / self.feature['enemyDamage']\n",
    "        self.feature['distancePerHeals'] = self.feature['totalDistance'] / self.feature['heals']\n",
    "        self.feature['headShotKillRatio'] = self.feature['headshotKills'] / self.feature['kills']\n",
    "        self.feature['headshotKillRate'] = self.feature['headshotKills'] / self.feature['kills']\n",
    "        self.feature['killPlaceOverMaxPlace'] = self.feature['killPlace'] / self.feature['maxPlace']\n",
    "        self.feature['kills/walkDistance'] = self.feature['kills'] / self.feature['walkDistance']\n",
    "        self.feature['avgKills'] = self.feature['squadKills'] / self.feature['teamPlayers']\n",
    "        self.feature['damageRatio'] = self.feature['damageDealt'] / self.feature['enemyDamage']\n",
    "        self.feature['distTravelledPerGame'] = self.feature['totalDistance'] / self.feature['matchDuration']\n",
    "        self.feature['killPlacePerc'] = self.feature['killPlace'] / self.feature['gamePlayers']\n",
    "        self.feature['playerSkill'] = self.feature['headshotKills'] + self.feature['roadKills'] + self.feature['assists'] - (5 * self.feature['teamKills'])\n",
    "        self.feature['gamePlacePerc'] = self.feature['killPlace'] / self.feature['maxPlace']\n",
    "    \n",
    "    def fillna(self):\n",
    "        self.feature.fillna(0, inplace=True)\n",
    "        self.feature.replace(np.inf, 0, inplace=True)\n",
    "        if self.is_train:\n",
    "            self.label.fillna(0, inplace=True)\n",
    "            self.label.replace(np.inf, 0, inplace=True)\n",
    "            \n",
    "#     def fillInf(self, val):\n",
    "#         numcols = self.feature.select_dtypes(include='number').columns\n",
    "#         cols = numcols[numcols != 'winPlacePerc']\n",
    "#         self.feature[self.feature == np.Inf] = np.NaN\n",
    "#         self.feature[self.feature == np.NINF] = np.NaN\n",
    "#         for c in cols:\n",
    "#             self.feature[c].fillna(val, inplace=True)\n",
    "        \n",
    "    def ohencode(self):\n",
    "        '''\n",
    "        solo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\n",
    "        duo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\n",
    "        squad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n",
    "        '''\n",
    "        mapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n",
    "        self.feature['matchType'] = self.feature['matchType'].apply(mapper)\n",
    "\n",
    "        self.feature = pd.concat([self.feature, pd.get_dummies(self.feature['matchType'], prefix='matchType')], axis=1)\n",
    "    \n",
    "    def drop_cols(self):\n",
    "        drop_cols = ['killPoints', 'rankPoints', 'winPoints', 'maxPlace']\n",
    "        self.feature.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    def drop_ids(self):\n",
    "        self.feature = self.feature.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1)\n",
    "    \n",
    "    def featuring(self):\n",
    "        features = list(self.feature.columns)\n",
    "        features.remove(\"Id\")\n",
    "        features.remove(\"matchId\")\n",
    "        features.remove(\"groupId\")\n",
    "        features.remove(\"matchType\")\n",
    "        condition='False'\n",
    "        \n",
    "        if 'winPlacePerc' in self.feature.columns:\n",
    "            y = np.array(self.feature.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean'), dtype=np.float64)\n",
    "            features.remove(\"winPlacePerc\")\n",
    "            condition='True'\n",
    "        \n",
    "        # get group mean feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = agg.reset_index()[['matchId','groupId']]\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "        # get group max feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('max')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "        \n",
    "        # get group min feature\n",
    "        agg = self.feature.groupby(['matchId','groupId'])[features].agg('min')\n",
    "        agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "        df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "        \n",
    "        # get match mean feature\n",
    "        agg = self.feature.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "        df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "        df_id=df_out[[\"matchId\", \"groupId\"]].copy()\n",
    "        df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "        del agg, agg_rank\n",
    "        gc.collect()\n",
    "        if condition == 'True':\n",
    "            return df_out,pd.DataFrame(y),df_id\n",
    "        else:\n",
    "            return df_out,df_id\n",
    "        \n",
    "    def split_t_v(self):\n",
    "        ts_f, vs_f, ts_l, vs_l = \\\n",
    "        train_test_split(\n",
    "            self.feature,\n",
    "            self.label,\n",
    "            test_size=0.1,\n",
    "            random_state=2\n",
    "        )\n",
    "        return ts_f, ts_l, vs_f, vs_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ts_f, ts_l, vs_f, vs_l,\n",
    "        batch_size=42\n",
    "    ):\n",
    "        self.ts_f = ts_f\n",
    "        self.ts_l = ts_l\n",
    "        self.vs_f = vs_f\n",
    "        self.vs_l = vs_l\n",
    "        self.estimator = tf.estimator.LinearRegressor(\n",
    "            feature_columns=self.get_feature_cols(),\n",
    "            model_dir='./test'\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        print('create model!')\n",
    "        \n",
    "    def get_feature_cols(self):\n",
    "        return set([tf.feature_column.numeric_column(my_feature) for my_feature in self.ts_f])\n",
    "        \n",
    "    def train(self):\n",
    "        train_input_fn = lambda: self.input_fn(self.ts_f, self.ts_l, batch_size=self.batch_size)\n",
    "        vali_input_fn = lambda: self.input_fn(self.vs_f, self.vs_l, num_epochs=1, shuffle=False)\n",
    "        for i in range(2):\n",
    "            self.estimator.train(input_fn=train_input_fn)\n",
    "            vali_eval = self.estimator.evaluate(input_fn=vali_input_fn)\n",
    "            print(f'epoch{i} ', vali_eval)\n",
    "    '''\n",
    "    # Use entire batch since this is such a small dataset.\n",
    "    NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "    def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "      def input_fn():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "        if shuffle:\n",
    "          dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "        # For training, cycle thru dataset as many times as need (n_epochs=None).    \n",
    "        dataset = (dataset\n",
    "          .repeat(n_epochs)\n",
    "          .batch(NUM_EXAMPLES)) \n",
    "        return dataset\n",
    "      return input_fn\n",
    "\n",
    "    # Training and evaluation input functions.\n",
    "    train_input_fn = make_input_fn(dftrain, y_train)\n",
    "    eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\n",
    "    '''\n",
    "#     def input_fn(\n",
    "#         self,\n",
    "# #         features_df,\n",
    "#         X,\n",
    "# #         targets_df,\n",
    "#         y,\n",
    "#         batch_size=1,\n",
    "#         shuffle=True,\n",
    "#         n_epochs=None\n",
    "#     ):\n",
    "#         NUM_EXAMPLES = batch_size#len(y)\n",
    "#         def input_fn():\n",
    "#             dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "#             if shuffle:\n",
    "#                 dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "#             # For training, cycle thru dataset as many times as need (n_epochs=None).    \n",
    "#             dataset = (\n",
    "#                 dataset\n",
    "#                 .repeat(n_epochs)\n",
    "#                 .batch(NUM_EXAMPLES)\n",
    "#             ) \n",
    "#             return dataset\n",
    "#         return input_fn\n",
    "\n",
    "\n",
    "    def input_fn(\n",
    "        self,\n",
    "        features_df,\n",
    "        targets_df,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_epochs=None\n",
    "    ):\n",
    "        # Convert pandas data into a dict of np arrays.\n",
    "        features = {key:np.array(value) for key,value in dict(features_df).items()}\n",
    "#         features = dict(features_df)\n",
    "    \n",
    "        # Construct a dataset, and configure batching/repeating.\n",
    "        ds = tf.data.Dataset.from_tensor_slices((features,targets_df)) # warning: 2GB limit\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        # Shuffle the data, if specified.\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)\n",
    "        # Return the next batch of data.\n",
    "#         features, labels = ds.make_one_shot_iterator().get_next()\n",
    "#         return features, labels\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4002269, 39)\n",
      "(4002269,)\n",
      "(444697, 39)\n",
      "(444697,)\n"
     ]
    }
   ],
   "source": [
    "pg_data = get_data(train_path)\n",
    "print(pg_data.ts_f.shape)\n",
    "print(pg_data.ts_l.shape)\n",
    "print(pg_data.vs_f.shape)\n",
    "print(pg_data.vs_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model!\n"
     ]
    }
   ],
   "source": [
    "pg_model = create_model(\n",
    "    ts_f=pg_data.ts_f,\n",
    "    ts_l=pg_data.ts_l,\n",
    "    vs_f=pg_data.vs_f,\n",
    "    vs_l=pg_data.vs_l\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0313 19:39:17.556583 140229344798464 deprecation.py:323] From /home/g42chsu/anaconda3/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2758: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0313 19:39:19.563575 140229344798464 deprecation.py:506] From /home/g42chsu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0313 20:34:27.101907 140229344798464 deprecation.py:323] From /home/g42chsu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:965: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "pg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
