{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Input, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "train_path = './data/train/audio/'\n",
    "test_path = './data/test/'\n",
    "train_words = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# t_data_dir = Path(train_path)\n",
    "# # print(t_data_dir)\n",
    "# files = [(str(file), file.parts[-2]) for file in t_data_dir.glob(\"**/*.wav\") if file]\n",
    "# # print(*files)\n",
    "# t_df = pd.DataFrame(files, columns=['path', 'word'])\n",
    "# t_df.head()\n",
    "\n",
    "# words = t_df['word'].unique().tolist()\n",
    "# # print(words)\n",
    "# silence = ['_background_noise_']\n",
    "# unknown = [word for word in words if word not in silence + train_words]\n",
    "# known = [word for word in words if word in silence + train_words]\n",
    "# # print(unknown)\n",
    "# # print(known)\n",
    "# tt_df = t_df.copy()\n",
    "# tt_df.loc[tt_df['word'].isin(silence), 'word'] = 'unknown'\n",
    "# tt_df.loc[tt_df['word'].isin(unknown), 'word'] = 'unknown'\n",
    "# tt_df.head()\n",
    "# labelbinarizer = LabelBinarizer()\n",
    "# label = labelbinarizer.fit_transform(tt_df['word'])\n",
    "# print(label)\n",
    "# print(len(label))\n",
    "# print(label[0])\n",
    "# print(tt_df['word'][0])\n",
    "# print(label[3000])\n",
    "# print(tt_df['word'][3000])\n",
    "# print(label[8000])\n",
    "# print(tt_df['word'][8000])\n",
    "# print(label[8000][4])\n",
    "# print(len(tt_df['word']))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 0:\n",
    "#     print(0)\n",
    "# else:\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_label_lst(df, labels):\n",
    "#     ret = {}\n",
    "#     tmp = {}\n",
    "    \n",
    "#     len_labels = len(labels)\n",
    "#     len_label = len(labels[0])\n",
    "#     for i in range(len_labels):\n",
    "#         for j in range(len_label):\n",
    "#             if labels[i][j] and j not in ret and df['word'][i] not in ret.values():\n",
    "#                 ret[j] = df['word'][i]\n",
    "#                 tmp[j] = i\n",
    "                \n",
    "#     print(ret)\n",
    "# #     print(tmp)\n",
    "#     arr = []\n",
    "#     for i in range(len(ret)):\n",
    "#         arr.append(ret[i])\n",
    "#     print(arr)\n",
    "\n",
    "\n",
    "# find_label_lst(tt_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = {}\n",
    "# arr[3] = 2\n",
    "# arr[1] = 5\n",
    "# arr[0] = 1\n",
    "# arr[2] = 3\n",
    "# if 5 in arr:\n",
    "#     arr[3] = 3\n",
    "\n",
    "# lst = []\n",
    "# for i in range(4):\n",
    "#     lst.append(arr[i])\n",
    "# print(arr)\n",
    "# print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, train_words, is_train=True):\n",
    "        self.path = path\n",
    "        self.train_words = train_words\n",
    "        self.get_data()\n",
    "        self.prepare_data()\n",
    "        if is_train:\n",
    "            self.split_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        words = self.df['word'].unique().tolist()\n",
    "        silence = ['_background_noise_']\n",
    "        unknown = [word for word in words if word not in silence + self.train_words]\n",
    "        # Mark silence files as unknown too\n",
    "        self.df.loc[self.df['word'].isin(silence), 'word'] = 'unknown'\n",
    "        self.df.loc[self.df['word'].isin(unknown), 'word'] = 'unknown'\n",
    "            \n",
    "    def get_data(self):\n",
    "        data_dir = Path(self.path)\n",
    "        files = [(str(file), file.parts[-2]) for file in data_dir.glob(\"**/*.wav\") if file]\n",
    "        self.df = pd.DataFrame(files, columns=['path', 'word'])\n",
    "#         print(self.df.isnull().values.any())\n",
    "        \n",
    "    def split_data(self):\n",
    "        labelbinarizer = LabelBinarizer()\n",
    "        self.X = self.df.path\n",
    "        self.y = labelbinarizer.fit_transform(self.df['word'])\n",
    "        self.ts_f, self.vs_f, self.ts_l, self.vs_l =\\\n",
    "        train_test_split(\n",
    "            self.X,\n",
    "            self.y,\n",
    "            test_size=0.2,\n",
    "            stratify=self.y\n",
    "        )\n",
    "        \n",
    "    def find_label_lst(self):\n",
    "        df = self.df\n",
    "        labels = self.y\n",
    "        ret = {}\n",
    "\n",
    "        len_labels = len(labels)\n",
    "        len_label = len(labels[0])\n",
    "        for i in range(len_labels):\n",
    "            for j in range(len_label):\n",
    "                if labels[i][j] and j not in ret and df['word'][i] not in ret.values():\n",
    "                    ret[j] = df['word'][i]  \n",
    "        arr = []\n",
    "        for i in range(len(ret)):\n",
    "            arr.append(ret[i])\n",
    "        self.pred_label = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_words,\n",
    "        shape=(129, 124, 1),\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        save_model_name='default',\n",
    "        load_model_name='default',\n",
    "        best_model_name='best_model'\n",
    "    ):\n",
    "        self.shape = shape\n",
    "        self.train_words = train_words\n",
    "        \n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_json_path = './model/' + save_model_name + '.json'\n",
    "        self.save_model_HDF5_path = './model/' + save_model_name + '.h5'\n",
    "        self.load_model_json_path = './model/' + load_model_name + '.json'\n",
    "        self.load_model_HDF5_path = './model/' + load_model_name + '.h5'\n",
    "        \n",
    "        if load_model:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.build()\n",
    "        self.compile_model()\n",
    "        print('create model !')\n",
    "        \n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.save_model_json_path, 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(self.save_model_HDF5_path)\n",
    "        print(\"Saving the model...\")\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            json_file = open(self.load_model_json_path, 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(self.load_model_HDF5_path)\n",
    "            print(\"Loaded model...\")\n",
    "        except:\n",
    "            print(\"Loading error!\")\n",
    "    \n",
    "    def build(self):\n",
    "        input_layer = Input(shape=self.shape)\n",
    "        \n",
    "        ''' basic one\n",
    "        model = BatchNormalization()(input_layer)\n",
    "        model = Conv2D(16, (3, 3), activation='elu')(model)\n",
    "        model = Dropout(0.25)(model)\n",
    "        model = MaxPooling2D((2, 2))(model)\n",
    "\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(32, activation='elu')(model)\n",
    "        model = Dropout(0.25)(model)\n",
    "        \n",
    "        model = Dense(len(self.train_words) + 1, activation='softmax')(model)\n",
    "        '''\n",
    "        \n",
    "        model = BatchNormalization()(input_layer)\n",
    "        \n",
    "        model = Conv2D(8, (2, 2), activation='relu')(model)\n",
    "        model = Conv2D(8, (2, 2), activation='relu')(model)\n",
    "        model = Dropout(0.2)(model)\n",
    "        model = MaxPooling2D((2, 2))(model)\n",
    "        \n",
    "        model = Conv2D(16, (3, 3), activation='relu')(model)\n",
    "        model = Conv2D(16, (3, 3), activation='relu')(model)\n",
    "        model = Dropout(0.2)(model)\n",
    "        model = MaxPooling2D((2, 2))(model)\n",
    "        \n",
    "        model = Conv2D(32, (3, 3), activation='relu')(model)\n",
    "        model = Dropout(0.2)(model)\n",
    "        model = MaxPooling2D((2, 2))(model)\n",
    "\n",
    "        model = Flatten()(model)\n",
    "        model = BatchNormalization()(Dense(128, activation='relu')(model))\n",
    "        model = BatchNormalization()(Dense(128, activation='relu')(model))\n",
    "        \n",
    "        # 11 because background noise has been taken out\n",
    "        model = Dense(len(self.train_words) + 1, activation='softmax')(model)\n",
    "\n",
    "        self.model = Model(inputs=input_layer, outputs=model)\n",
    "        \n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        ts_f,\n",
    "        ts_l,\n",
    "        vs_f,\n",
    "        vs_l,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        has_tb=False,\n",
    "        tb_path='./tensorboard/test_v0',\n",
    "    ):\n",
    "        self.ts_f = ts_f\n",
    "        self.ts_l = ts_l\n",
    "        self.vs_f = vs_f\n",
    "        self.vs_l = vs_l\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.has_tb = has_tb\n",
    "        self.tb_path = tb_path\n",
    "        \n",
    "        self.train_gen = self.batch_generator(self.ts_f.values, self.ts_l, self.batch_size)\n",
    "        self.vali_gen = self.batch_generator(self.vs_f.values, self.vs_l, self.batch_size)\n",
    "        tensorboard = TensorBoard(\n",
    "            log_dir=self.tb_path,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        \n",
    "        fit_arg = dict(\n",
    "            generator=self.train_gen,\n",
    "            epochs=self.epochs,\n",
    "            steps_per_epoch=self.ts_f.shape[0] // self.batch_size,\n",
    "            validation_data=self.vali_gen,\n",
    "            validation_steps=self.vs_f.shape[0] // self.batch_size\n",
    "        )\n",
    "        if self.has_tb:\n",
    "            fit_arg['callbacks'] = [tensorboard]\n",
    "        if self.save_model:\n",
    "            \n",
    "        self.model.fit_generator(**fit_arg)\n",
    "        if self.save_model:\n",
    "            self.save()\n",
    "    \n",
    "    def batch_generator(self, X, y, batch_size=16):\n",
    "        # Return a random image from X, y\n",
    "        while True:\n",
    "            idx_lst = np.random.randint(0, X.shape[0], batch_size)\n",
    "            imgs = X[idx_lst]\n",
    "            labels = y[idx_lst]\n",
    "            spgs = self.get_spectrogram(imgs)\n",
    "            \n",
    "            yield np.concatenate([spgs]), labels\n",
    "            \n",
    "    def get_spectrogram(self, paths, nsamples=16000):\n",
    "        # Given list of paths, return spectrogram\n",
    "#         for path in paths:\n",
    "#             print(path)\n",
    "        wavs = [wavfile.read(path)[1] for path in paths]\n",
    "        \n",
    "        data = []\n",
    "        for wav in wavs:\n",
    "            if wav.size < 16000:\n",
    "                d = np.pad(wav, (nsamples - wav.size, 0), mode='constant')\n",
    "            else:\n",
    "                d = wav[0:nsamples]\n",
    "            data.append(d)\n",
    "            \n",
    "        spg = [signal.spectrogram(d, nperseg=256, noverlap=128)[2] for d in data]\n",
    "        spg = [s.reshape(129, 124, -1) for s in spg]\n",
    "        return (spg)\n",
    "    \n",
    "#     def predict(self, test_df):\n",
    "#         self.predictions = []\n",
    "#         paths = test_df['path'].tolist()\n",
    "        \n",
    "#         for path in paths:\n",
    "#             spg = self.get_spectrogram([path])\n",
    "#             pred = self.model.predict(np.array(spg))\n",
    "#             self.predictions.extend(pred)\n",
    "            \n",
    "#         labelbinarizer = LabelBinarizer()\n",
    "#         labels = [labelbinarizer.inverse_transform(p.reshape(1, -1), threshold=0.5)[0] for p in self.predictions]\n",
    "#         test_df['labels'] = labels\n",
    "#         test_df['path'] = test_df['path'].apply(lambda x: str(x).split('/')[-1])\n",
    "#         self.submission = pd.DataFrame(\n",
    "#             {\n",
    "#                 'fname': test_df['path'].tolist(),\n",
    "#                 'label': labels\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "    def predict(self, test_df, pred_label):\n",
    "        self.predictions = []\n",
    "        paths = test_df['path'].tolist()\n",
    "        len_paths = len(paths)\n",
    "        for i in range(len_paths):\n",
    "            spg = self.get_spectrogram([paths[i]])\n",
    "            pred = self.model.predict(np.array(spg))\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            pred = pred_label[pred[0]]\n",
    "            self.predictions.append(pred)\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"{i}/{len_paths}\")\n",
    "\n",
    "        test_df['pred'] = self.predictions\n",
    "        test_df['fname'] = test_df['path'].apply(lambda x: str(x).split('/')[-1])\n",
    "        self.submission = pd.DataFrame(\n",
    "            {\n",
    "                'fname': test_df['fname'].tolist(),\n",
    "                'label': test_df['pred'].tolist()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def save_predict(self, path='submission_v2.csv'):\n",
    "        self.submission.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(train_path, train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'unknown', 'up', 'yes']\n"
     ]
    }
   ],
   "source": [
    "train.find_label_lst()\n",
    "print(train.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model...\n",
      "create model !\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    train_words,\n",
    "#     save_model=True,\n",
    "#     save_model_name='default_8epos'\n",
    "    load_model=True,\n",
    "    load_model_name='default_8epos'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0328 04:22:47.776945 140125303052032 callbacks.py:1197] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1618/1618 [==============================] - 471s 291ms/step - loss: 0.1721 - accuracy: 0.9398 - val_loss: 0.1804 - val_accuracy: 0.9310\n",
      "Epoch 2/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0905 - accuracy: 0.9664 - val_loss: 0.0895 - val_accuracy: 0.9664\n",
      "Epoch 3/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.0739 - val_accuracy: 0.9726\n",
      "Epoch 4/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0554 - accuracy: 0.9797 - val_loss: 0.0715 - val_accuracy: 0.9735\n",
      "Epoch 5/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0480 - accuracy: 0.9825 - val_loss: 0.0581 - val_accuracy: 0.9797\n",
      "Epoch 6/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0436 - accuracy: 0.9843 - val_loss: 0.0600 - val_accuracy: 0.9787\n",
      "Epoch 7/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0396 - accuracy: 0.9857 - val_loss: 0.0534 - val_accuracy: 0.9800\n",
      "Epoch 8/8\n",
      "1618/1618 [==============================] - 466s 288ms/step - loss: 0.0355 - accuracy: 0.9875 - val_loss: 0.0497 - val_accuracy: 0.9823\n",
      "Saving the model...\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train.ts_f,\n",
    "    train.ts_l,\n",
    "    train.vs_f,\n",
    "    train.vs_l,\n",
    "    epochs=8,\n",
    "    has_tb=True,\n",
    "    tb_path='./tensorboard/test_v8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 21:20:10.632958 139889126586112 callbacks.py:1197] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 165/1618 [==>...........................] - ETA: 10:38 - loss: 0.0382 - accuracy: 0.9862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g42chsu/anaconda3/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 587/1618 [=========>....................] - ETA: 6:46 - loss: 0.0381 - accuracy: 0.9864"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-69e87ad96406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhas_tb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtb_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./tensorboard/test_v9'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-73-7e6a1ba9a106>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ts_f, ts_l, vs_f, vs_l, epochs, batch_size, has_tb, tb_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_tb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mfit_arg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train.ts_f,\n",
    "    train.ts_l,\n",
    "    train.vs_f,\n",
    "    train.vs_l,\n",
    "    epochs=1,\n",
    "    has_tb=False,\n",
    "    tb_path='./tensorboard/test_v9'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a8a262403b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-7e6a1ba9a106>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(self, X, y, batch_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mspgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-7e6a1ba9a106>\u001b[0m in \u001b[0;36mget_spectrogram\u001b[0;34m(self, paths, nsamples)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m#         for path in paths:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m#             print(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mwavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-7e6a1ba9a106>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m#         for path in paths:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m#             print(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mwavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_big_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mfmt_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mstr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'RIFF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mis_big_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(*model.train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data(test_path, train_words, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/test/audio/clip_0201b98c2.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/test/audio/clip_78befd0e0.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/test/audio/clip_0e7824991.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/test/audio/clip_773b7caf8.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/test/audio/clip_f14da8108.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 path     word\n",
       "0  data/test/audio/clip_0201b98c2.wav  unknown\n",
       "1  data/test/audio/clip_78befd0e0.wav  unknown\n",
       "2  data/test/audio/clip_0e7824991.wav  unknown\n",
       "3  data/test/audio/clip_773b7caf8.wav  unknown\n",
       "4  data/test/audio/clip_f14da8108.wav  unknown"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/158538\n",
      "1000/158538\n",
      "2000/158538\n",
      "3000/158538\n",
      "4000/158538\n",
      "5000/158538\n",
      "6000/158538\n",
      "7000/158538\n",
      "8000/158538\n",
      "9000/158538\n",
      "10000/158538\n",
      "11000/158538\n",
      "12000/158538\n",
      "13000/158538\n",
      "14000/158538\n",
      "15000/158538\n",
      "16000/158538\n",
      "17000/158538\n",
      "18000/158538\n",
      "19000/158538\n",
      "20000/158538\n",
      "21000/158538\n",
      "22000/158538\n",
      "23000/158538\n",
      "24000/158538\n",
      "25000/158538\n",
      "26000/158538\n",
      "27000/158538\n",
      "28000/158538\n",
      "29000/158538\n",
      "30000/158538\n",
      "31000/158538\n",
      "32000/158538\n",
      "33000/158538\n",
      "34000/158538\n",
      "35000/158538\n",
      "36000/158538\n",
      "37000/158538\n",
      "38000/158538\n",
      "39000/158538\n",
      "40000/158538\n",
      "41000/158538\n",
      "42000/158538\n",
      "43000/158538\n",
      "44000/158538\n",
      "45000/158538\n",
      "46000/158538\n",
      "47000/158538\n",
      "48000/158538\n",
      "49000/158538\n",
      "50000/158538\n",
      "51000/158538\n",
      "52000/158538\n",
      "53000/158538\n",
      "54000/158538\n",
      "55000/158538\n",
      "56000/158538\n",
      "57000/158538\n",
      "58000/158538\n",
      "59000/158538\n",
      "60000/158538\n",
      "61000/158538\n",
      "62000/158538\n",
      "63000/158538\n",
      "64000/158538\n",
      "65000/158538\n",
      "66000/158538\n",
      "67000/158538\n",
      "68000/158538\n",
      "69000/158538\n",
      "70000/158538\n",
      "71000/158538\n",
      "72000/158538\n",
      "73000/158538\n",
      "74000/158538\n",
      "75000/158538\n",
      "76000/158538\n",
      "77000/158538\n",
      "78000/158538\n",
      "79000/158538\n",
      "80000/158538\n",
      "81000/158538\n",
      "82000/158538\n",
      "83000/158538\n",
      "84000/158538\n",
      "85000/158538\n",
      "86000/158538\n",
      "87000/158538\n",
      "88000/158538\n",
      "89000/158538\n",
      "90000/158538\n",
      "91000/158538\n",
      "92000/158538\n",
      "93000/158538\n",
      "94000/158538\n",
      "95000/158538\n",
      "96000/158538\n",
      "97000/158538\n",
      "98000/158538\n",
      "99000/158538\n",
      "100000/158538\n",
      "101000/158538\n",
      "102000/158538\n",
      "103000/158538\n",
      "104000/158538\n",
      "105000/158538\n",
      "106000/158538\n",
      "107000/158538\n",
      "108000/158538\n",
      "109000/158538\n",
      "110000/158538\n",
      "111000/158538\n",
      "112000/158538\n",
      "113000/158538\n",
      "114000/158538\n",
      "115000/158538\n",
      "116000/158538\n",
      "117000/158538\n",
      "118000/158538\n",
      "119000/158538\n",
      "120000/158538\n",
      "121000/158538\n",
      "122000/158538\n",
      "123000/158538\n",
      "124000/158538\n",
      "125000/158538\n",
      "126000/158538\n",
      "127000/158538\n",
      "128000/158538\n",
      "129000/158538\n",
      "130000/158538\n",
      "131000/158538\n",
      "132000/158538\n",
      "133000/158538\n",
      "134000/158538\n",
      "135000/158538\n",
      "136000/158538\n",
      "137000/158538\n",
      "138000/158538\n",
      "139000/158538\n",
      "140000/158538\n",
      "141000/158538\n",
      "142000/158538\n",
      "143000/158538\n",
      "144000/158538\n",
      "145000/158538\n",
      "146000/158538\n",
      "147000/158538\n",
      "148000/158538\n",
      "149000/158538\n",
      "150000/158538\n",
      "151000/158538\n",
      "152000/158538\n",
      "153000/158538\n",
      "154000/158538\n",
      "155000/158538\n",
      "156000/158538\n",
      "157000/158538\n",
      "158000/158538\n"
     ]
    }
   ],
   "source": [
    "model.predict(test.df, train.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fname    label\n",
       "0   NaN  unknown\n",
       "1   NaN  unknown\n",
       "2   NaN  unknown\n",
       "3   NaN       no\n",
       "4   NaN  unknown"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_v0 = model.submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_v0['fname'] = test.df['path'].apply(lambda x: str(x).split('/')[-1])\n",
    "submit_v0.to_csv('submission_v0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "# paths = test.df['path'].tolist()\n",
    "# path = paths[3000]\n",
    "\n",
    "path = train.df['path'][3000]\n",
    "\n",
    "# print(path)\n",
    "spg = model.get_spectrogram([path])\n",
    "# print(spg)\n",
    "pred = model.model.predict(np.array(spg))\n",
    "pred = np.argmax(pred, axis=1)\n",
    "# pred = [lab]\n",
    "print(pred)\n",
    "print(train.pred_label[pred[0]])\n",
    "# pridictions = []\n",
    "# for path in paths:\n",
    "#     spg = model.get_spectrogram([path])\n",
    "#     pred = model.model.predict(np.array(spg))\n",
    "#     predictions.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>data/test/audio/clip_ab0bcda7b.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>data/test/audio/clip_ff0e145d3.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>data/test/audio/clip_c119f835f.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>data/test/audio/clip_277b78495.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>data/test/audio/clip_8aea89f4b.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    path     word\n",
       "5000  data/test/audio/clip_ab0bcda7b.wav  unknown\n",
       "5001  data/test/audio/clip_ff0e145d3.wav  unknown\n",
       "5002  data/test/audio/clip_c119f835f.wav  unknown\n",
       "5003  data/test/audio/clip_277b78495.wav  unknown\n",
       "5004  data/test/audio/clip_8aea89f4b.wav  unknown"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_point = 5000\n",
    "t_df = test.df.iloc[s_point : s_point + 5,]\n",
    "t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
