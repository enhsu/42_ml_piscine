{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n",
      "Number of training examples: 784\n",
      "Number of cross-validation examples = 784\n",
      "x_train shape: (37800, 784)\n",
      "y_train shape: (37800, 10)\n",
      "y_train sample0: [0 0 0 0 0 0 0 1 0 0]\n",
      "x_test shape: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow as tf\n",
    "import math\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "trainset = pd.read_csv('./data/train.csv')\n",
    "testset = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(trainset.shape)\n",
    "print(testset.shape)\n",
    "\n",
    "# split dataset into tainset and cross-validation set\n",
    "trainset, valset= train_test_split(trainset, test_size = 0.1)\n",
    "\n",
    "# get pix from trainset\n",
    "x_train = trainset.loc[:, \"pixel0\" : \"pixel783\"]\n",
    "x_train = x_train.values\n",
    "# get label from trainset\n",
    "y_train = trainset.loc[:,\"label\"]\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "\n",
    "# get pix from valset\n",
    "x_cv = valset.loc[:, \"pixel0\" : \"pixel783\"]\n",
    "x_cv = x_cv.values\n",
    "# get label from valset\n",
    "y_cv = valset.loc[:,\"label\"]\n",
    "y_cv = pd.get_dummies(y_cv).values\n",
    "\n",
    "x_test = testset.values\n",
    "\n",
    "print(\"Number of training examples: \" + str(x_train.shape[1]))\n",
    "print(\"Number of cross-validation examples = \" + str(x_cv.shape[1]))\n",
    "print (\"x_train shape: \" + str(x_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"y_train sample0: \" + str(y_train[0]))\n",
    "print (\"x_test shape: \" + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x_shape, y_shape, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    ops.reset_default_graph()      # reset computation graph\n",
    "    \"\"\"\n",
    "    x_shape: input dim\n",
    "    y_shape: output dim\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32, shape=(None, x_shape), name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, y_shape), name=\"y\")\n",
    "    \n",
    "    # flatten the input\n",
    "    print(x)\n",
    "    m = tf.layers.dense(x, units=256, activation='relu',\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(m)\n",
    "    m = tf.layers.dropout(inputs=m, rate=0.4)\n",
    "    print(m)\n",
    "    m = tf.layers.dense(m, units=128, activation='relu',\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(m)\n",
    "#     m = tf.layers.dropout(inputs=m, rate=0.4)\n",
    "#     print(m)\n",
    "    m = tf.layers.dense(m, units=64, activation='relu', \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(m)\n",
    "#     m = tf.layers.dropout(inputs=m, rate=0.4)\n",
    "#     print(m)\n",
    "    m = tf.layers.dense(m, units=32, activation='relu', \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(m)\n",
    "#     m = tf.layers.dropout(inputs=m, rate=0.4)\n",
    "#     print(m)\n",
    "    m = tf.layers.dense(m, units=16, activation='relu', \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(m)\n",
    "#     m = tf.layers.dropout(inputs=m, rate=0.4)\n",
    "#     print(m)\n",
    "    prediction = tf.layers.dense(m, units=y_shape, name=\"p\", \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        bias_initializer=tf.zeros_initializer())\n",
    "    print(prediction)\n",
    "    \n",
    "    return x, y, prediction\n",
    "\n",
    "def optimization(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: pred value\n",
    "    labels: real value\n",
    "    \"\"\"\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    optim = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "#     optim = tf.contrib.opt.NadamOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "    return loss, optim\n",
    "\n",
    "def random_mini_batches(x, y, mini_batch_size = 64):\n",
    "    m = x.shape[0]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #shuffle x and y\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_x = x[permutation]\n",
    "    shuffled_y = y[permutation]\n",
    "    \n",
    "    #partition\n",
    "    num_complete_minibatches = math.ceil(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_x = shuffled_x[k*mini_batch_size : k*mini_batch_size + mini_batch_size]\n",
    "        mini_batch_y = shuffled_y[k*mini_batch_size : k*mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "def train(x_train, y_train, x_cv, y_cv, x_test, num_epochs, mini_batch_size = 64):\n",
    "    x_shape = x_train.shape\n",
    "    y_shape = y_train.shape\n",
    "    print(\"input: \", x_shape[1], \" output: \", y_shape[1])\n",
    "    x, y, pred = network(x_shape[1], y_shape[1])\n",
    "    # loss, optim\n",
    "    loss, optim = optimization(pred, y)\n",
    "    # acc \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # init tensorflow\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)  #initializes the variables created\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0\n",
    "            epoch_acc = 0\n",
    "            num_minibatches = math.ceil(x_shape[0]/mini_batch_size)\n",
    "            minibatches = random_mini_batches(x_train, y_train)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_x, minibatch_y) = minibatch\n",
    "                _, minibatch_cost, p, minibatch_acc  = sess.run([optim, loss, pred, accuracy], feed_dict = {x: minibatch_x, y: minibatch_y})\n",
    "                # print(\"pred shape: \", p)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                epoch_acc += minibatch_acc / num_minibatches\n",
    "                \n",
    "            print(\"cost after epoch %i :  %.3f\" % (epoch + 1, epoch_cost), end=\"\")\n",
    "            print(\"  train accuracy   :  %.3f\" % epoch_acc)\n",
    "            print(\"  cv accuracy   :  %.3f\" % (accuracy.eval({x: x_cv, y: y_cv})))\n",
    "        print(\"network trained\")\n",
    "        predicts = tf.argmax(pred, 1).eval({x:x_test})\n",
    "        probs = tf.nn.softmax(pred, 1).eval({x: x_test})\n",
    "        print(\"test shape: \", x_test.shape)\n",
    "        print(\"predicts shape: \", predicts.shape)\n",
    "        print(\"predicts val: \", predicts)\n",
    "        return predicts, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  784  output:  10\n",
      "Tensor(\"x:0\", shape=(?, 784), dtype=float32)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-d6987978b3f9>:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"dense/Relu:0\", shape=(?, 256), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-2-d6987978b3f9>:16: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Tensor(\"dropout/Identity:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dense_1/Relu:0\", shape=(?, 128), dtype=float32)\n",
      "Tensor(\"dense_2/Relu:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"dense_3/Relu:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_4/Relu:0\", shape=(?, 16), dtype=float32)\n",
      "Tensor(\"p/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "cost after epoch 1 :  0.332  train accuracy   :  0.589\n",
      "  cv accuracy   :  0.875\n",
      "cost after epoch 2 :  0.062  train accuracy   :  0.912\n",
      "  cv accuracy   :  0.936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7bc9cbd44e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d6987978b3f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, x_cv, y_cv, x_test, num_epochs, mini_batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mnum_minibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mminibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mminibatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d6987978b3f9>\u001b[0m in \u001b[0;36mrandom_mini_batches\u001b[0;34m(x, y, mini_batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#shuffle x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mshuffled_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds , probs = train(x_train, y_train, x_cv, y_cv, x_test, num_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
