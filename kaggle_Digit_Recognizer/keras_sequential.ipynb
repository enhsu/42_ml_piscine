{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reference: https://www.kaggle.com/bulentsiyah/mnist-for-beginners-tensorflow-dnn-cnn-keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for tensorboard\n",
    "import os\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "\n",
    "# reference - custom model example\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/iris_custom_model.py\n",
    "\n",
    "# https://gist.github.com/damienpontifex/1f03b966d36049b678efdddb54cef4eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # load csv file\n",
    "        data = pd.read_csv(path)\n",
    "        # split data to feature & label\n",
    "        if is_train:\n",
    "            self.feature = data.drop('label', axis=1)\n",
    "            self.label = data['label']\n",
    "        else:\n",
    "            self.feature = data\n",
    "        # free space\n",
    "        del data\n",
    "        \n",
    "        self.feature = self.deal_feature()\n",
    "        if is_train:\n",
    "            self.label = self.deal_label()\n",
    "            self.ts_f, self.ts_l, self.vs_f, self.vs_l = self.split_t_v()\n",
    "            \n",
    "    \n",
    "    # EDA\n",
    "    def describe_plot(self):\n",
    "        sns.countplot(self.label)\n",
    "    def describe_num(self):\n",
    "        res = self.label.value_counts()\n",
    "        print(res)\n",
    "    \n",
    "    # reference: How to Check If Any Value is NaN in a Pandas DataFrame\n",
    "    # https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/\n",
    "    def check_missing_val(self, data_df):\n",
    "        res = data_df.isnull().values.any()\n",
    "        print(res)\n",
    "        \n",
    "    def deal_feature(self):\n",
    "        # normolize data\n",
    "#         feature = self.feature / 255.0\n",
    "        feature = self.feature.values.reshape(-1, 28, 28, 1)\n",
    "        print(feature)\n",
    "        return feature\n",
    "    \n",
    "    def show_digit(self):\n",
    "        plt.imshow(self.feature[0][:,:,0])\n",
    "        \n",
    "    def deal_label(self):\n",
    "        label = to_categorical(self.label, num_classes = 10)\n",
    "#         1, [0, 1, 0, 0, ... ]\n",
    "        print(label)\n",
    "        return label\n",
    "    \n",
    "    def split_t_v(self):\n",
    "        # trainingSet_feature\n",
    "        # trainingSet_label\n",
    "        # validationSet_feature\n",
    "        # validationSet_label\n",
    "        \n",
    "        ts_f, ts_l, vs_f, vs_l = \\\n",
    "        train_test_split(\n",
    "            self.feature,\n",
    "            self.label,\n",
    "            test_size=0.1,\n",
    "            random_state=2\n",
    "        )\n",
    "        return ts_f, ts_l, vs_f, vs_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainValTensorBoard(TensorBoard):\n",
    "#     def __init__(self, log_dir='./logs', **kwargs):\n",
    "#         self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "#         training_log_dir = os.path.join(log_dir, 'training')\n",
    "#         super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "#     def set_model(self, model):\n",
    "#         if context.executing_eagerly():\n",
    "#             self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
    "#         else:\n",
    "#             self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "#         super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "#     def _write_custom_summaries(self, step, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
    "#         if context.executing_eagerly():\n",
    "#             with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "#                 for name, value in val_logs.items():\n",
    "#                     tf.contrib.summary.scalar(name, value.item(), step=step)\n",
    "#         else:\n",
    "#             for name, value in val_logs.items():\n",
    "#                 summary = tf.Summary()\n",
    "#                 summary_value = summary.value.add()\n",
    "#                 summary_value.simple_value = value.item()\n",
    "#                 summary_value.tag = name\n",
    "#                 self.val_writer.add_summary(summary, step)\n",
    "#         self.val_writer.flush()\n",
    "\n",
    "#         logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
    "#         super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
    "\n",
    "#     def on_train_end(self, logs=None):\n",
    "#         super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "#         self.val_writer.close()\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        print(logs)\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=42,\n",
    "        num_classes=10,\n",
    "        epochs=10,\n",
    "        input_shape=(28, 28, 1),\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        save_model_name='default',\n",
    "        load_model_name='default',\n",
    "        tensorboard=True\n",
    "    ):\n",
    "        self.batch_size=batch_size\n",
    "        self.num_classes=num_classes\n",
    "        self.epochs=epochs\n",
    "        self.input_shape=input_shape\n",
    "        \n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_json_path = './model/' + save_model_name + '.json'\n",
    "        self.save_model_HDF5_path = './model/' + save_model_name + '.h5'\n",
    "        self.load_model_json_path = './model/' + load_model_name + '.json'\n",
    "        self.load_model_HDF5_path = './model/' + load_model_name + '.h5'\n",
    "        self.has_tb = tensorboard\n",
    "        if self.has_tb:\n",
    "            log_dir = './tensorboard/{}'.format(time())\n",
    "            self.tensorboard = TrainValTensorBoard(log_dir=log_dir, write_graph=False)\n",
    "        \n",
    "        if load_model:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.buildDNN()\n",
    "        self.create_optimizer()\n",
    "        self.model_compile()\n",
    "        \n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.save_model_json_path, 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(self.save_model_HDF5_path)\n",
    "        print(\"Saving the model...\")\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            json_file = open(self.load_model_json_path, 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(self.load_model_HDF5_path)\n",
    "            print(\"Loaded model...\")\n",
    "        except:\n",
    "            print(\"Loading error!\")\n",
    "            \n",
    "        \n",
    "    def buildDNN(self):\n",
    "        self.model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))\n",
    "        self.model.add(MaxPool2D((2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))\n",
    "        self.model.add(MaxPool2D((2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    def model_compile(self):\n",
    "        self.model.compile(\n",
    "            optimizer=self.adam,\n",
    "            loss=keras.losses.categorical_crossentropy,\n",
    "            metrics=['accuracy', self.recall]\n",
    "        )\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        self.sgd = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "        self.rms_prop = keras.optimizers.RMSprop(lr=1e-4)\n",
    "        self.adam = keras.optimizers.adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "        self.adamax = keras.optimizers.Adamax(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "        self.adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    def recall(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def train(self, ts_f, ts_l, vs_f, vs_l):\n",
    "        self.create_learning_rate_reduction()\n",
    "        self.get_datagenn(ts_f)\n",
    "        \n",
    "        cbs = [self.learning_rate_reduction]\n",
    "        if self.has_tb:\n",
    "            cbs.append(self.tensorboard)\n",
    "        \n",
    "        self.history = self.model.fit_generator(\n",
    "            self.datagenn.flow(\n",
    "                ts_f,\n",
    "                vs_f,\n",
    "                batch_size=self.batch_size\n",
    "            ),\n",
    "            epochs=self.epochs,\n",
    "            validation_data=(ts_l,vs_l),\n",
    "            verbose=2,\n",
    "            steps_per_epoch=ts_f.shape[0] // self.batch_size,\n",
    "            callbacks=cbs,\n",
    "        )\n",
    "        self.result = self.model.evaluate(x=ts_f, y=vs_f)\n",
    "        print('Accuracy:', self.result[1])\n",
    "        if self.save_model:\n",
    "            self.save()\n",
    "        self.print_plot()\n",
    "    \n",
    "    def create_learning_rate_reduction(self):\n",
    "        self.learning_rate_reduction = ReduceLROnPlateau(\n",
    "            monitor='val_acc', \n",
    "            patience=3, \n",
    "            verbose=1, \n",
    "            factor=0.5, \n",
    "            min_lr=0.0001\n",
    "        )\n",
    "\n",
    "    def get_datagenn(self, ts_f):\n",
    "        self.datagenn = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.1, # Randomly zoom image \n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False\n",
    "        )  # randomly flip images\n",
    "\n",
    "        self.datagenn.fit(ts_f)\n",
    "    \n",
    "    def print_plot(self):\n",
    "        self.print_loss()\n",
    "        self.print_acc()\n",
    "    \n",
    "    def print_loss(self):\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.title(\"Model Loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Test'])\n",
    "        plt.show()\n",
    "        \n",
    "    def print_acc(self):\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(['Train','Test'])\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, test):\n",
    "        pred_digits_test = np.argmax(self.model.predict(test), axis=1)\n",
    "        image_id_test=[]\n",
    "        for i in range(len(pred_digits_test)):\n",
    "            image_id_test.append(i+1)\n",
    "        d = {\n",
    "            'ImageId': image_id_test,\n",
    "            'Label': pred_digits_test\n",
    "        }\n",
    "        this.answer = pd.DataFrame(d)\n",
    "        \n",
    "    def save_pred(self, path):\n",
    "        this.answer.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]]\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0e33c6844a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mts_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvs_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mvs_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvs_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-6-01cb96810470>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ts_f, ts_l, vs_f, vs_l)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mts_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mts_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model: keras_sequential\n",
    "train = get_data(train_path)\n",
    "model_seq = create_model(\n",
    "    batch_size=10,\n",
    "#     num_classes=10,\n",
    "    epochs=2,\n",
    "#     input_shape=(28, 28, 1),\n",
    "#     save_model=True,\n",
    "#     load_model=True,\n",
    "#     save_model_name='default',\n",
    "#     load_model_name='default',\n",
    "#     tensorboard=True\n",
    ")\n",
    "model_seq.train(\n",
    "    ts_f=train.ts_f,\n",
    "    ts_l=train.ts_l,\n",
    "    vs_f=train.vs_f,\n",
    "    vs_l=train.vs_l\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
