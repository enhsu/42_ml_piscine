{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import things\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, path, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # load csv file\n",
    "        data = pd.read_csv(path)\n",
    "        # split data to feature & label\n",
    "        if is_train:\n",
    "            self.feature = data.drop('label', axis=1)\n",
    "            self.label = data['label']\n",
    "        else:\n",
    "            self.feature = data\n",
    "        # free space\n",
    "        del data\n",
    "        \n",
    "        self.feature = self.deal_feature()\n",
    "        if is_train:\n",
    "            self.label = self.deal_label()\n",
    "            self.ts_f, self.ts_l, self.vs_f, self.vs_l = self.split_t_v()\n",
    "            \n",
    "    \n",
    "    # EDA\n",
    "    def describe_plot(self):\n",
    "        sns.countplot(self.label)\n",
    "    def describe_num(self):\n",
    "        res = self.label.value_counts()\n",
    "        print(res)\n",
    "    \n",
    "    # reference: How to Check If Any Value is NaN in a Pandas DataFrame\n",
    "    # https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/\n",
    "    def check_missing_val(self, data_df):\n",
    "        res = data_df.isnull().values.any()\n",
    "        print(res)\n",
    "        \n",
    "    def deal_feature(self):\n",
    "        # normolize data\n",
    "#         feature = self.feature / 255.0\n",
    "        feature = self.feature.values.reshape(-1, 28, 28, 1)\n",
    "        return feature\n",
    "    \n",
    "    def show_digit(self):\n",
    "        plt.imshow(self.feature[0][:,:,0])\n",
    "        \n",
    "    def deal_label(self):\n",
    "        label = to_categorical(self.label, num_classes = 10)\n",
    "        return label\n",
    "    \n",
    "    def split_t_v(self):\n",
    "        # trainingSet_feature\n",
    "        # trainingSet_label\n",
    "        # validationSet_feature\n",
    "        # validationSet_label\n",
    "        \n",
    "        ts_f, ts_l, vs_f, vs_l = \\\n",
    "        train_test_split(\n",
    "            self.feature,\n",
    "            self.label,\n",
    "            test_size=0.1,\n",
    "            random_state=2\n",
    "        )\n",
    "        return ts_f, ts_l, vs_f, vs_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_data:\n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path)\n",
    "#         self.missing_data = self.get_missing()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   class create_model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df,\n",
    "        label_col='label',\n",
    "        learning_rate=0.02,\n",
    "        steps=100,\n",
    "        batch_size=10,\n",
    "        periods=10,\n",
    "        hidden_units=[512, 126, 64],\n",
    "        load_model=False,\n",
    "        load_model_name='default',\n",
    "        save_model=False,\n",
    "        save_model_name='default',\n",
    "    ):\n",
    "        self.label = label_col\n",
    "        # learning rate: optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        # steps, batch_size, periods: train\n",
    "        self.steps = steps\n",
    "        self.batch_size = batch_size\n",
    "        self.periods = periods\n",
    "        self.h_units = hidden_units\n",
    "        # save model\n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.save_model_name = save_model_name\n",
    "        self.load_model_name = load_model_name\n",
    "        self.save_model_path = './model/' + self.save_model_name\n",
    "        self.load_model_path = './model/' + self.load_model_name\n",
    "        # split data\n",
    "        self.te, self.tt, self.ve, self.vt = self.get_split_data(train_df)\n",
    "        \n",
    "        # create lenearRegressor\n",
    "        self.feature_cols = self.get_feature_cols(train_df)\n",
    "        self.optimizer = self.get_optimizer(learning_rate)\n",
    "        \n",
    "        if self.load_model:\n",
    "            if self.save_model:\n",
    "                self.lr = tf.estimator.DNNClassifier(\n",
    "                    hidden_units=self.h_units,\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    model_dir=self.save_model_path,\n",
    "                    warm_start_from=self.load_model_path,\n",
    "                    n_classes=10\n",
    "                )\n",
    "            else:\n",
    "                self.lr = tf.estimator.DNNClassifier(\n",
    "                    hidden_units=self.h_units,\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    warm_start_from=self.load_model_path,\n",
    "                    n_classes=10\n",
    "                )\n",
    "        else:\n",
    "            if self.save_model:\n",
    "                self.lr = tf.estimator.DNNClassifier(\n",
    "                    hidden_units=self.h_units,\n",
    "                #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    model_dir=self.save_model_path,\n",
    "                    n_classes=10\n",
    "                ) \n",
    "            else:\n",
    "                self.lr = tf.estimator.DNNClassifier(\n",
    "                    hidden_units=self.h_units,\n",
    "    #             self.lr = tf.estimator.LinearRegressor(\n",
    "                    feature_columns=self.feature_cols,\n",
    "                    optimizer=self.optimizer,\n",
    "                    n_classes=10\n",
    "                )\n",
    "        print('build the model')\n",
    "    \n",
    "    def get_split_data(self, train_df):\n",
    "        # sample 80% for train data, 20% for vali data\n",
    "        train_set, vali_set = self.split_train(train_df, 0.8)\n",
    "        te, tt = self.get_e_t(train_set)\n",
    "        ve, vt = self.get_e_t(vali_set)\n",
    "        return te, tt, ve, vt\n",
    "        \n",
    "    def split_train(self, data_df, per):\n",
    "        t_s = data_df.sample(frac=per, replace=False, random_state=42)\n",
    "        v_s = data_df.loc[ set(data_df.index) - set(t_s.index)]\n",
    "        return t_s, v_s\n",
    "    \n",
    "    def get_e_t(self, data_df):\n",
    "        # data examples\n",
    "        d_e = data_df.copy().drop(self.label, axis=1)\n",
    "        # data targets\n",
    "        d_t = pd.DataFrame()\n",
    "        d_t[self.label] = data_df[self.label]\n",
    "        return d_e, d_t\n",
    "    \n",
    "    def get_feature_cols(self, train_df):\n",
    "        feature_df = train_df.copy().drop(self.label, axis=1)\n",
    "        tmp_feature = [tf.feature_column.numeric_column(my_feature) for my_feature in feature_df]\n",
    "#         if bin_age: # boolean:\n",
    "#             tmp_age = tf.feature_column.numeric_column(\"Age\")\n",
    "#             bucketized_age = tf.feature_column.bucketized_column(\n",
    "#               tmp_age, boundaries=get_quantile_based_boundaries(\n",
    "#                 input_features[\"Age\"], 4))\n",
    "#             tmp_feature += [bucketized_age]\n",
    "        return set(tmp_feature)\n",
    "    \n",
    "    def get_optimizer(self, learning_rate):\n",
    "#         change the optimizer!!!!!!\n",
    "        my_opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#         my_opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        my_opt = tf.contrib.estimator.clip_gradients_by_norm(my_opt, 2.0)\n",
    "        return my_opt\n",
    "    \n",
    "    def my_input_fn(\n",
    "        self,\n",
    "        features,\n",
    "        targets,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_epochs=None\n",
    "    ):\n",
    "        # Convert pandas data into a dict of np arrays.\n",
    "        features = {key:np.array(value) for key,value in dict(features).items()}\n",
    "        # Construct a dataset, and configure batching/repeating.\n",
    "        ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        # Shuffle the data, if specified.\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "        # Return the next batch of data.\n",
    "        features, labels = ds.make_one_shot_iterator().get_next()\n",
    "        return features, labels\n",
    "    \n",
    "    def train(self):\n",
    "        steps_per_period = self.steps / self.periods\n",
    "        # create input function\n",
    "        training_input_fn = lambda: self.my_input_fn(self.te, self.tt[self.label], batch_size=self.batch_size)\n",
    "        predict_training_input_fn = lambda: self.my_input_fn(self.te, self.tt[self.label], num_epochs=1, shuffle=False)\n",
    "        predict_vali_input_fn = lambda: self.my_input_fn(self.ve, self.vt[self.label], num_epochs=1, shuffle=False)\n",
    "        \n",
    "        print('Training model...')\n",
    "        # recording RMSE\n",
    "        training_rmse = []\n",
    "        validation_rmse = []\n",
    "        for period in range(self.periods):\n",
    "            self.lr.train(\n",
    "                input_fn=training_input_fn,\n",
    "                steps=steps_per_period\n",
    "            )\n",
    "            self.lr.evaluate(\n",
    "                input_fn=predict_vali_input_fn,\n",
    "                steps=steps_per_period\n",
    "            )\n",
    "            \n",
    "            # compute training predictions\n",
    "            training_predictions = self.lr.predict(input_fn=predict_training_input_fn)\n",
    "            training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "            # compute validation predictions\n",
    "            validation_predictions = self.lr.predict(input_fn=predict_vali_input_fn)\n",
    "            validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "            # get validation eval\n",
    "            training_eval = self.lr.evaluate(input_fn=predict_training_input_fn)\n",
    "            validation_eval = self.lr.evaluate(input_fn=predict_vali_input_fn)\n",
    "            print(\"training_eval: {}\".format(training_eval['average_loss']))\n",
    "            print(\"validation_eval: {}\".format(validation_eval['average_loss']))\n",
    "\n",
    "            # compute training loss\n",
    "            training_root_mean_squared_error = math.sqrt(\n",
    "                metrics.mean_squared_error(training_predictions, self.tt))\n",
    "            # compute validation loss\n",
    "            validation_root_mean_squared_error = math.sqrt(\n",
    "                metrics.mean_squared_error(validation_predictions, self.vt))\n",
    "            # Occasionally print the current loss.\n",
    "            print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
    "            # Add the loss metrics from this period to our list.\n",
    "            training_rmse.append(training_root_mean_squared_error)\n",
    "            validation_rmse.append(validation_root_mean_squared_error)\n",
    "        \n",
    "        print(\"Model training finished.\")\n",
    "        # saving model\n",
    "        if self.save_model:\n",
    "            print(\"Saving model...\")\n",
    "            \n",
    "        # output a graph of loss metrics over periods.\n",
    "        self.result_plot(training_rmse, validation_rmse)\n",
    "        \n",
    "    def result_plot(self, t_rmse, v_rmse):\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.xlabel(\"Periods\")\n",
    "        plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "        plt.tight_layout()\n",
    "        plt.plot(t_rmse, label=\"training\")\n",
    "        plt.plot(v_rmse, label=\"validation\")\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "class predict_file:\n",
    "    def __init__(self, model, test_df, id_frame):\n",
    "        self.test_df = test_df\n",
    "        self.id_frame = id_frame\n",
    "#         self.predict_col = predict_col\n",
    "        self.predict_input_fn = self.create_predict_fn()\n",
    "        \n",
    "        self.predictions = list(model.lr.predict(input_fn=self.predict_input_fn))\n",
    "        self.predictions = np.array([item['predictions'][0] for item in self.predictions])\n",
    "        self.predictions *= 1000\n",
    "        \n",
    "        self.evaluation = self.id_frame#test_df['Id'].copy().to_frame()\n",
    "        self.evaluation['SalePrice'] = self.predictions\n",
    "    \n",
    "    def create_predict_fn(self):\n",
    "#         predict_df = self.test_df\n",
    "        predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "            x=self.test_df,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return predict_input_fn\n",
    "    \n",
    "    def save_predict(self, path):\n",
    "        self.evaluation.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
